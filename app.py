"""
Aplicativo Streamlit para consulta de informa√ß√µes sobre a C√¢mara dos Deputados
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import streamlit as st
import faiss
import pickle
import numpy as np
from utils.embeddings import EmbeddingManager
import logging
import google.generativeai as genai
from dotenv import load_dotenv
from pathlib import Path
import glob

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Carrega vari√°veis de ambiente
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

def get_available_indices():
    """Retorna lista de √≠ndices dispon√≠veis"""
    embeddings_dir = Path("data/embeddings")
    index_files = glob.glob(str(embeddings_dir / "*_index.faiss"))
    indices = [Path(f).stem.replace("_index", "") for f in index_files]
    return sorted(indices)

def load_index_and_data(name):
    """Carrega √≠ndice FAISS e dados originais"""
    # Carrega dados
    with open(f"data/embeddings/{name}_data.pkl", "rb") as f:
        data = pickle.load(f)
    
    # Carrega √≠ndice
    index = faiss.read_index(f"data/embeddings/{name}_index.faiss")
    
    return data, index

def search_all_indices(manager, query, k=10):
    """Realiza busca em todos os √≠ndices dispon√≠veis"""
    indices = get_available_indices()
    all_results = []
    
    # Gera embedding da query uma √∫nica vez
    query_emb = manager.gerar_embeddings_batch([query])[0].reshape(1, -1)
    
    # Mapeamento de √≠ndices para prioridade
    priority_map = {
        'insights_distribuicao': 1,
        'insights_despesas': 1,
        'sumarizacoes': 2,
        'deputados': 3,
        'despesas': 3,
        'proposicoes': 3
    }
    
    for index_name in indices:
        try:
            data, index = load_index_and_data(index_name)
            
            # Ajusta k baseado na prioridade do √≠ndice
            index_priority = priority_map.get(index_name, 3)
            index_k = k * index_priority
            
            # Busca similares
            D, I = index.search(query_emb, index_k)
            
            # Adiciona resultados v√°lidos
            for dist, idx in zip(D[0], I[0]):
                if idx < len(data):
                    # Ajusta o score baseado na prioridade do √≠ndice
                    adjusted_score = float(dist) / index_priority
                    all_results.append({
                        'text': data[idx],
                        'score': adjusted_score,
                        'source': index_name
                    })
        except Exception as e:
            logger.error(f"Erro ao buscar no √≠ndice {index_name}: {e}")
            continue
    
    # Ordena todos os resultados por score ajustado
    all_results.sort(key=lambda x: x['score'])
    
    # Retorna mais resultados, mas mant√©m um limite razo√°vel
    return all_results[:min(len(all_results), 30)]

def format_context(results):
    """Formata o contexto de forma estruturada"""
    context_by_source = {}
    
    # Agrupa resultados por fonte
    for result in results:
        source = result['source']
        if source not in context_by_source:
            context_by_source[source] = []
        context_by_source[source].append(result['text'])
    
    # Formata o contexto
    formatted_parts = []
    
    # Primeiro insights e sumariza√ß√µes
    for source in ['insights_distribuicao', 'insights_despesas', 'sumarizacoes']:
        if source in context_by_source:
            formatted_parts.append(f"\n=== {source.upper()} ===\n")
            formatted_parts.extend(context_by_source[source])
            del context_by_source[source]
    
    # Depois os dados espec√≠ficos
    for source, texts in context_by_source.items():
        formatted_parts.append(f"\n=== {source.upper()} ===\n")
        formatted_parts.extend(texts)
    
    return "\n".join(formatted_parts)

def get_gemini_response(query, context):
    """Gera resposta usando Gemini"""
    model = genai.GenerativeModel('gemini-pro')
    
    SYSTEM_PROMPT = """Voc√™ √© um especialista em an√°lise de dados parlamentares com vasta experi√™ncia em interpretar informa√ß√µes da C√¢mara dos Deputados.
    Como analista experiente, voc√™ deve usar a t√©cnica Self-Ask para estruturar seu racioc√≠nio:
    
    1. T√âCNICA SELF-ASK:
    - Fa√ßa perguntas espec√≠ficas para si mesmo sobre o problema
    - Responda cada pergunta usando os dados dispon√≠veis
    - Use as respostas para construir seu racioc√≠nio
    - Questione suas pr√≥prias conclus√µes
    
    Exemplo de Self-Ask:
    Q: Quais dados s√£o necess√°rios para responder esta pergunta?
    A: Preciso de X, Y e Z...
    
    Q: Estes dados est√£o dispon√≠veis no contexto?
    A: Sim/N√£o, encontrei/n√£o encontrei...
    
    Q: Que an√°lises posso fazer com estes dados?
    A: Posso comparar X com Y...
    
    Q: Esta conclus√£o √© suportada pelos dados?
    A: Sim/N√£o, porque...
    
    2. AN√ÅLISE DE DADOS:
    - Analise criticamente os dados dispon√≠veis
    - Identifique padr√µes e tend√™ncias relevantes
    - Fa√ßa infer√™ncias razo√°veis quando apropriado
    - Use seu conhecimento especializado para contextualizar
    
    3. COMUNICA√á√ÉO:
    - Estruture sua resposta de forma clara
    - Explique seu racioc√≠nio passo a passo
    - Destaque limita√ß√µes e incertezas
    - Forne√ßa exemplos espec√≠ficos dos dados
    
    Lembre-se: Use Self-Ask para guiar seu pensamento e tornar seu racioc√≠nio transparente."""

    USER_PROMPT = """Pergunta: {query}

    Contexto dispon√≠vel:
    {context}

    Use a t√©cnica Self-Ask para analisar a pergunta:

    1. QUEST√ïES INICIAIS:
    - Do que precisamente trata esta pergunta?
    - Quais dados s√£o necess√°rios para respond√™-la?
    - Estes dados est√£o dispon√≠veis no contexto?

    2. AN√ÅLISE DOS DADOS:
    - Que padr√µes posso identificar nos dados?
    - Como posso verificar se estes padr√µes s√£o significativos?
    - Que conclus√µes posso tirar com seguran√ßa?

    3. VALIDA√á√ÉO:
    - Minhas conclus√µes s√£o suportadas pelos dados?
    - Existem limita√ß√µes importantes a considerar?
    - Que ressalvas preciso fazer?

    4. RESPOSTA FINAL:
    Estruture sua resposta incluindo:
    - Processo de racioc√≠nio usado
    - Dados e an√°lises realizadas
    - Conclus√µes e limita√ß√µes
    - Recomenda√ß√µes, se apropriado"""

    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT.format(query=query, context=context)
    
    response = model.generate_content(prompt)
    return response.text

def main():
    st.title("üèõÔ∏è Assistente Virtual da C√¢mara dos Deputados")
    
    st.markdown("""
    Este assistente pode responder perguntas sobre:
    - üë• Deputados e partidos
    - üí∞ Despesas e gastos
    - üìú Proposi√ß√µes e projetos de lei
    - üìä An√°lises e insights
    """)
    
    # Inicializa o hist√≥rico de chat se n√£o existir
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Mostra hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Campo de entrada do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta..."):
        # Adiciona pergunta do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Mostra indicador de "pensando"
        with st.chat_message("assistant"):
            with st.spinner("Buscando informa√ß√µes..."):
                # Inicializa manager
                manager = EmbeddingManager()
                
                # Busca em todos os √≠ndices
                results = search_all_indices(manager, prompt)
                
                # Formata o contexto de forma estruturada
                context = format_context(results)
                
                # Gera resposta
                response = get_gemini_response(prompt, context)
                
                # Adiciona resposta ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.markdown(response)
                
                # Opcionalmente, mostra o contexto usado (para debug)
                if st.checkbox("Mostrar contexto usado"):
                    st.text_area("Contexto", context, height=300)

if __name__ == "__main__":
    main()
