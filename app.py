"""
Aplicativo Streamlit para consulta de informa√ß√µes sobre a C√¢mara dos Deputados
"""
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import streamlit as st
import faiss
import pickle
import numpy as np
from utils.embeddings import EmbeddingManager
import logging
import google.generativeai as genai
from dotenv import load_dotenv
from pathlib import Path
import glob

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Carrega vari√°veis de ambiente
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

def get_available_indices():
    """Retorna lista de √≠ndices dispon√≠veis"""
    embeddings_dir = Path("data/embeddings")
    index_files = glob.glob(str(embeddings_dir / "*_index.faiss"))
    indices = [Path(f).stem.replace("_index", "") for f in index_files]
    return sorted(indices)

def load_index_and_data(name):
    """Carrega √≠ndice FAISS e dados originais"""
    # Carrega dados
    with open(f"data/embeddings/{name}_data.pkl", "rb") as f:
        data = pickle.load(f)
    
    # Carrega √≠ndice
    index = faiss.read_index(f"data/embeddings/{name}_index.faiss")
    
    return data, index

def search_all_indices(manager, query, k=10):
    """Realiza busca em todos os √≠ndices dispon√≠veis"""
    indices = get_available_indices()
    all_results = []
    
    # Gera embedding da query uma √∫nica vez
    query_emb = manager.gerar_embeddings_batch([query])[0].reshape(1, -1)
    
    # Mapeamento de √≠ndices para prioridade
    priority_map = {
        'insights_distribuicao': 0.5,  # Maior prioridade (score menor = mais relevante)
        'insights_despesas': 1,
        'sumarizacoes': 1.5,
        'deputados': 2,
        'despesas': 2,
        'proposicoes': 2
    }
    
    for index_name in indices:
        try:
            data, index = load_index_and_data(index_name)
            
            # Ajusta k baseado na prioridade do √≠ndice
            index_priority = priority_map.get(index_name, 3)
            index_k = k * index_priority
            
            # Busca similares
            D, I = index.search(query_emb, index_k)
            
            # Adiciona resultados v√°lidos
            for dist, idx in zip(D[0], I[0]):
                if idx < len(data):
                    # Ajusta o score baseado na prioridade do √≠ndice
                    adjusted_score = float(dist) / index_priority
                    all_results.append({
                        'text': data[idx],
                        'score': adjusted_score,
                        'source': index_name
                    })
        except Exception as e:
            logger.error(f"Erro ao buscar no √≠ndice {index_name}: {e}")
            continue
    
    # Ordena todos os resultados por score ajustado
    all_results.sort(key=lambda x: x['score'])
    
    # Retorna mais resultados, mas mant√©m um limite razo√°vel
    return all_results[:min(len(all_results), 30)]

def format_context(results):
    """Formata o contexto de forma estruturada"""
    context_by_source = {}
    
    # Agrupa resultados por fonte
    for result in results:
        source = result['source']
        if source not in context_by_source:
            context_by_source[source] = []
        context_by_source[source].append(result['text'])
    
    # Formata o contexto
    formatted_parts = []
    
    # Primeiro os insights de distribui√ß√£o (mais importantes)
    if 'insights_distribuicao' in context_by_source:
        formatted_parts.append("\n=== DISTRIBUI√á√ÉO PARTID√ÅRIA (AN√ÅLISE GERAL) ===\n")
        formatted_parts.extend(context_by_source['insights_distribuicao'])
        del context_by_source['insights_distribuicao']
    
    # Depois outros insights e sumariza√ß√µes
    for source in ['insights_despesas', 'sumarizacoes']:
        if source in context_by_source:
            formatted_parts.append(f"\n=== {source.upper()} ===\n")
            formatted_parts.extend(context_by_source[source])
            del context_by_source[source]
    
    # Por fim, dados espec√≠ficos
    for source, texts in context_by_source.items():
        formatted_parts.append(f"\n=== DADOS ESPEC√çFICOS: {source.upper()} ===\n")
        formatted_parts.extend(texts)
    
    return "\n".join(formatted_parts)

def get_gemini_response(query, context):
    """Gera resposta usando Gemini"""
    model = genai.GenerativeModel('gemini-pro')
    
    SYSTEM_PROMPT = """Voc√™ √© um especialista em an√°lise de dados parlamentares com vasta experi√™ncia em interpretar informa√ß√µes da C√¢mara dos Deputados.
    Como analista experiente, voc√™ deve usar a t√©cnica Self-Ask para estruturar seu racioc√≠nio, seguindo uma an√°lise hier√°rquica:

    1. AN√ÅLISE HIER√ÅRQUICA COM SELF-ASK:
    
    N√≠vel 1 - An√°lise Geral:
    Q: Existe uma an√°lise geral ou distribui√ß√£o global no contexto?
    A: Procure na se√ß√£o "DISTRIBUI√á√ÉO PARTID√ÅRIA (AN√ÅLISE GERAL)"
    
    Q: Quais s√£o os principais n√∫meros e percentuais dessa an√°lise?
    A: Liste os n√∫meros encontrados
    
    N√≠vel 2 - Valida√ß√£o:
    Q: Os dados espec√≠ficos confirmam a an√°lise geral?
    A: Compare com dados espec√≠ficos se necess√°rio
    
    Q: Existem discrep√¢ncias ou pontos a esclarecer?
    A: Identifique poss√≠veis inconsist√™ncias
    
    N√≠vel 3 - Conclus√£o:
    Q: Qual √© a resposta mais precisa baseada em todos os dados?
    A: Combine an√°lise geral com valida√ß√µes espec√≠ficas
    
    2. REGRAS DE AN√ÅLISE:
    - SEMPRE comece pela an√°lise geral/distribui√ß√£o global
    - Use dados espec√≠ficos apenas para valida√ß√£o
    - Cite n√∫meros e percentuais exatos quando dispon√≠veis
    - Indique claramente a fonte dos dados (geral ou espec√≠fica)
    
    3. ESTRUTURA DA RESPOSTA:
    - Comece com a conclus√£o principal (baseada na an√°lise geral)
    - Forne√ßa os n√∫meros exatos e percentuais
    - Adicione contexto e valida√ß√µes
    - Mencione limita√ß√µes apenas se relevantes
    
    Lembre-se: A an√°lise geral e distribui√ß√µes globais s√£o mais confi√°veis que exemplos espec√≠ficos."""

    USER_PROMPT = """Pergunta: {query}

    Contexto dispon√≠vel:
    {context}

    Use a t√©cnica Self-Ask com an√°lise hier√°rquica:

    1. AN√ÅLISE DA DISTRIBUI√á√ÉO GERAL:
    Q: O que mostra a an√°lise geral de distribui√ß√£o partid√°ria?
    A: [Procure na se√ß√£o espec√≠fica]
    
    Q: Quais s√£o os n√∫meros e percentuais principais?
    A: [Liste os dados encontrados]

    2. VALIDA√á√ÉO COM DADOS ESPEC√çFICOS:
    Q: Os dados espec√≠ficos confirmam a an√°lise geral?
    A: [Compare se necess√°rio]
    
    Q: Existem informa√ß√µes adicionais relevantes?
    A: [Identifique dados complementares]

    3. CONCLUS√ÉO FINAL:
    - Apresente a conclus√£o baseada principalmente na an√°lise geral
    - Cite n√∫meros e percentuais exatos
    - Adicione contexto relevante
    - Mencione fonte dos dados"""

    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT.format(query=query, context=context)
    
    response = model.generate_content(prompt)
    return response.text

def main():
    st.title("üèõÔ∏è Assistente Virtual da C√¢mara dos Deputados")
    
    st.markdown("""
    Este assistente pode responder perguntas sobre:
    - üë• Deputados e partidos
    - üí∞ Despesas e gastos
    - üìú Proposi√ß√µes e projetos de lei
    - üìä An√°lises e insights
    """)
    
    # Inicializa o hist√≥rico de chat se n√£o existir
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Mostra hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Campo de entrada do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta..."):
        # Adiciona pergunta do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Mostra indicador de "pensando"
        with st.chat_message("assistant"):
            with st.spinner("Buscando informa√ß√µes..."):
                # Inicializa manager
                manager = EmbeddingManager()
                
                # Busca em todos os √≠ndices
                results = search_all_indices(manager, prompt)
                
                # Formata o contexto de forma estruturada
                context = format_context(results)
                
                # Gera resposta
                response = get_gemini_response(prompt, context)
                
                # Adiciona resposta ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.markdown(response)
                
                # Opcionalmente, mostra o contexto usado (para debug)
                if st.checkbox("Mostrar contexto usado"):
                    st.text_area("Contexto", context, height=300)

if __name__ == "__main__":
    main()
