"""
Aplicativo Streamlit para consulta de informa√ß√µes sobre a C√¢mara dos Deputados
"""
import os
import sys
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# Adiciona o diret√≥rio raiz ao path para importar utils corretamente
root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(root_dir)

import streamlit as st
import faiss
import pickle
import numpy as np
import google.generativeai as genai
import logging
from pathlib import Path
from app.utils.embeddings import EmbeddingManager

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Assistente Virtual - C√¢mara dos Deputados",
    page_icon="üèõÔ∏è",
    layout="wide"
)

def get_available_indices():
    """Retorna lista de √≠ndices dispon√≠veis"""
    index_files = Path(f"{root_dir}/data/embeddings").glob("*_index.faiss")
    indices = [Path(f).stem.replace("_index", "") for f in index_files]
    return sorted(indices)

def load_index_and_data(name):
    """Carrega √≠ndice FAISS e dados originais"""
    # Carrega dados
    with open(f"{root_dir}/data/embeddings/{name}_data.pkl", "rb") as f:
        data = pickle.load(f)
    
    # Carrega √≠ndice
    index = faiss.read_index(f"{root_dir}/data/embeddings/{name}_index.faiss")
    
    return data, index

def search_all_indices(manager, query, k=10):
    """Realiza busca em todos os √≠ndices dispon√≠veis"""
    indices = get_available_indices()
    all_results = []
    
    # Gera embedding da query uma √∫nica vez
    query_emb = manager.gerar_embeddings_batch([query])[0].reshape(1, -1)
    
    # Mapeamento de √≠ndices para prioridade (score menor = mais relevante)
    priority_map = {
        'sumarizacoes': 0.5,  # Maior prioridade para sumariza√ß√µes
        'proposicoes': 1,     # Segunda maior prioridade para proposi√ß√µes espec√≠ficas
        'insights_distribuicao': 2,
        'insights_despesas': 2,
        'deputados': 3,
        'despesas': 3
    }
    
    # Ajusta k baseado no tipo de consulta
    if "proposi√ß√£o" in query.lower() or "proposicoes" in query.lower():
        k = k * 2  # Dobra o n√∫mero de resultados para consultas sobre proposi√ß√µes
    
    for index_name in indices:
        try:
            data, index = load_index_and_data(index_name)
            
            # Ajusta k baseado na prioridade do √≠ndice
            index_priority = priority_map.get(index_name, 3)
            index_k = k * (1 + (1/index_priority))  # Mais resultados para √≠ndices priorit√°rios
            
            # Busca similares
            D, I = index.search(query_emb, min(index_k, len(data)))
            
            # Adiciona resultados v√°lidos
            for dist, idx in zip(D[0], I[0]):
                if idx < len(data):
                    # Ajusta o score baseado na prioridade do √≠ndice
                    adjusted_score = float(dist) * index_priority
                    all_results.append({
                        'text': data[idx],
                        'score': adjusted_score,
                        'source': index_name
                    })
        except Exception as e:
            logging.error(f"Erro ao buscar no √≠ndice {index_name}: {e}")
            continue
    
    # Ordena todos os resultados por score ajustado
    all_results.sort(key=lambda x: x['score'])
    
    # Retorna mais resultados para dar contexto suficiente
    return all_results[:min(len(all_results), 40)]

def format_context(results):
    """Formata o contexto de forma estruturada"""
    context_by_source = {}
    
    # Agrupa resultados por fonte
    for result in results:
        source = result['source']
        if source not in context_by_source:
            context_by_source[source] = []
        context_by_source[source].append(result['text'])
    
    # Formata o contexto
    formatted_parts = []
    
    # Primeiro as sumariza√ß√µes
    if 'sumarizacoes' in context_by_source:
        formatted_parts.append("\n=== SUMARIZA√á√ïES DE PROPOSI√á√ïES ===\n")
        formatted_parts.extend(context_by_source['sumarizacoes'])
        del context_by_source['sumarizacoes']
    
    # Depois proposi√ß√µes espec√≠ficas
    if 'proposicoes' in context_by_source:
        formatted_parts.append("\n=== PROPOSI√á√ïES ESPEC√çFICAS ===\n")
        formatted_parts.extend(context_by_source['proposicoes'])
        del context_by_source['proposicoes']
    
    # Depois outros insights
    for source in ['insights_distribuicao', 'insights_despesas']:
        if source in context_by_source:
            formatted_parts.append(f"\n=== INSIGHTS: {source.upper()} ===\n")
            formatted_parts.extend(context_by_source[source])
            del context_by_source[source]
    
    # Por fim, dados complementares
    for source, texts in context_by_source.items():
        formatted_parts.append(f"\n=== DADOS COMPLEMENTARES: {source.upper()} ===\n")
        formatted_parts.extend(texts)
    
    return "\n".join(formatted_parts)

def get_gemini_response(query, context):
    """Gera resposta usando Gemini"""
    model = genai.GenerativeModel('gemini-pro')
    
    SYSTEM_PROMPT = """Voc√™ √© um especialista em an√°lise legislativa da C√¢mara dos Deputados, com foco especial em proposi√ß√µes e seus impactos.
    Como analista experiente, voc√™ deve usar a t√©cnica Self-Ask para estruturar seu racioc√≠nio:

    1. AN√ÅLISE HIER√ÅRQUICA COM SELF-ASK:
    
    N√≠vel 1 - An√°lise das Sumariza√ß√µes:
    Q: Existem sumariza√ß√µes relevantes sobre o tema no contexto?
    A: Procure na se√ß√£o "SUMARIZA√á√ïES" ou resumos gerais
    
    Q: Quais s√£o os principais pontos dessas sumariza√ß√µes?
    A: Liste os temas, quantidades e destaques encontrados
    
    N√≠vel 2 - An√°lise das Proposi√ß√µes Espec√≠ficas:
    Q: Quais proposi√ß√µes espec√≠ficas tratam do tema?
    A: Identifique n√∫meros e conte√∫dos relevantes
    
    Q: Como essas proposi√ß√µes se relacionam com a sumariza√ß√£o?
    A: Compare proposi√ß√µes espec√≠ficas com o resumo geral
    
    N√≠vel 3 - S√≠ntese e Impactos:
    Q: Quais s√£o os principais impactos esperados?
    A: Analise as consequ√™ncias potenciais
    
    Q: Existem padr√µes ou tend√™ncias relevantes?
    A: Identifique temas recorrentes
    
    2. REGRAS DE AN√ÅLISE:
    - Comece SEMPRE pelas sumariza√ß√µes gerais
    - Cite n√∫meros espec√≠ficos de proposi√ß√µes quando relevante
    - Identifique temas recorrentes e padr√µes
    - Destaque impactos potenciais na sociedade
    
    3. ESTRUTURA DA RESPOSTA:
    - Vis√£o Geral (baseada nas sumariza√ß√µes)
    - Proposi√ß√µes Espec√≠ficas (n√∫meros e conte√∫dos)
    - Temas Recorrentes
    - Impactos Esperados
    
    Lembre-se: Mantenha o foco nas proposi√ß√µes e seus impactos."""

    USER_PROMPT = """Pergunta: {query}

    Contexto dispon√≠vel:
    {context}

    Use a t√©cnica Self-Ask para analisar as proposi√ß√µes:

    1. AN√ÅLISE DAS SUMARIZA√á√ïES:
    Q: O que mostram as sumariza√ß√µes sobre este tema?
    A: [Analise os resumos gerais]
    
    Q: Quais s√£o os principais pontos e n√∫meros?
    A: [Liste os dados encontrados]

    2. AN√ÅLISE DAS PROPOSI√á√ïES:
    Q: Quais proposi√ß√µes espec√≠ficas s√£o relevantes?
    A: [Identifique n√∫meros e conte√∫dos]
    
    Q: Como elas se relacionam com o tema?
    A: [Analise a relev√¢ncia]

    3. S√çNTESE E IMPACTOS:
    - Apresente os principais achados
    - Destaque proposi√ß√µes espec√≠ficas
    - Identifique padr√µes
    - Analise impactos potenciais"""

    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT.format(query=query, context=context)
    
    response = model.generate_content(prompt)
    return response.text

def main():
    st.title("üèõÔ∏è Assistente Virtual da C√¢mara dos Deputados")
    
    st.markdown("""
    Este assistente pode responder perguntas sobre:
    - üë• Deputados e partidos
    - üí∞ Despesas e gastos
    - üìú Proposi√ß√µes e projetos de lei
    - üìä An√°lises e insights
    """)
    
    # Inicializa o hist√≥rico de chat se n√£o existir
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Mostra hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Campo de entrada do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta..."):
        # Adiciona pergunta do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Mostra indicador de "pensando"
        with st.chat_message("assistant"):
            with st.spinner("Buscando informa√ß√µes..."):
                # Inicializa manager
                manager = EmbeddingManager()
                
                # Busca em todos os √≠ndices
                results = search_all_indices(manager, prompt)
                
                # Formata o contexto de forma estruturada
                context = format_context(results)
                
                # Gera resposta
                response = get_gemini_response(prompt, context)
                
                # Adiciona resposta ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.markdown(response)
                
                # Opcionalmente, mostra o contexto usado (para debug)
                if st.checkbox("Mostrar contexto usado"):
                    st.text_area("Contexto", context, height=300)

if __name__ == "__main__":
    # Carrega vari√°veis de ambiente
    from dotenv import load_dotenv
    load_dotenv()
    
    # Configura Google API
    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
    
    main()
